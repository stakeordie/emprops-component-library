#!/bin/bash

# Base directory for your ComfyUI instances
ROOT="${ROOT:-/workspace}"

# Source environment variables
if [ -f /etc/environment ]; then
    while IFS='=' read -r key value; do
        if [ -n "$key" ]; then
            # Remove any leading/trailing whitespace and quotes
            key=$(echo "$key" | tr -d '"' | xargs)
            value=$(echo "$value" | tr -d '"' | xargs)
            export "$key=$value"
        fi
    done < /etc/environment
fi

log() {
    echo "[MGPU] $*" >&2
}

# Add debug logging at start (only once)
if [ "${DEBUG:-}" = "true" ]; then
    log "Debug: Environment variables at script start:"
    log "NUM_GPUS=${NUM_GPUS:-not set}"
    log "ROOT=$ROOT"
    log "PATH=$PATH"
    log "MOCK_GPU=${MOCK_GPU:-not set}"
fi

# Validate GPU ID
validate_gpu_id() {
    local gpu_id=$1

    # Special case for 'all'
    if [ "$gpu_id" = "all" ]; then
        [ "${DEBUG:-}" = "true" ] && log "Debug: Validating GPU ID: all"
        return 0
    fi

    # Check if gpu_id is a number
    if ! [[ "$gpu_id" =~ ^[0-9]+$ ]]; then
        log "Error: Invalid GPU ID '$gpu_id'. Must be a number or 'all'"
        return 1
    fi

    [ "${DEBUG:-}" = "true" ] && log "Debug: Validating GPU ID: $gpu_id"
    [ "${DEBUG:-}" = "true" ] && log "Debug: Current NUM_GPUS value: $NUM_GPUS"

    # Check if gpu_id is within range
    if [ "$gpu_id" -ge "${NUM_GPUS:-0}" ]; then
        log "Error: GPU ID '$gpu_id' is out of range. Must be between 0 and $((NUM_GPUS-1))"
        return 1
    fi

    return 0
}

# Function to show logs for a specific GPU
show_logs() {
    local gpu_id=$1
    local lines=${2:-5}  # Default to last 5 lines
    
    if [ "$gpu_id" = "all" ]; then
        show_all_logs "$lines"
        return
    fi
     
    local log_file="${ROOT}/comfyui_gpu${gpu_id}/logs/output.log"
    if [ -f "$log_file" ]; then
        # Only show errors and last few lines
        grep -i "error\|exception\|failed" "$log_file" | tail -n 3
        tail -n 3 "$log_file"
    else
        log "ERROR: Log file not found: $log_file"
        return 1
    fi
}

# Function to show logs for all GPUs
show_all_logs() {
    local lines=${1:-5}  # Default to last 5 lines
    for gpu in $(seq 0 $((NUM_GPUS-1))); do
        log "GPU $gpu:"
        show_logs "$gpu" "$lines"
    done
}

# Function to start a specific GPU service
start_service() {
    local gpu_id=$1
    
    if [ "$gpu_id" = "all" ]; then
        start_all_services
        return $?
    fi
    
    log "Starting service for GPU $gpu_id..."
    # Pass test_gpus as third parameter if MOCK_GPU is set
    if [ "${MOCK_GPU:-0}" -eq 1 ]; then
        service comfyui start "$gpu_id" "1" || return 1
    else
        service comfyui start "$gpu_id" || return 1
    fi
}

# Function to start all GPU services
start_all_services() {
    log "Starting all GPU services..."
    local failed=0
    for gpu in $(seq 0 $((NUM_GPUS-1))); do
        # Pass test_gpus as third parameter if MOCK_GPU is set
        if [ "${MOCK_GPU:-0}" -eq 1 ]; then
            start_service "$gpu" || failed=1
        else
            start_service "$gpu" || failed=1
        fi
    done
    return $failed
}

# Function to stop a specific GPU service
stop_service() {
    local gpu_id=$1
    
    if [ "$gpu_id" = "all" ]; then
        stop_all_services
        return
    fi
    
    log "Stopping service for GPU $gpu_id..."
    service comfyui stop "$gpu_id"
}

# Function to stop all GPU services
stop_all_services() {
    local success=true
    for gpu in $(seq 0 $((NUM_GPUS-1))); do
        if ! stop_service "$gpu"; then
            success=false
        fi
    done
    return $success
}

## Function to restart a specific GPU service
restart_service() {
    local gpu_id=$1
    
    if [ "$gpu_id" = "all" ]; then
        restart_all_services
        return
    fi
    
    # Validate GPU ID first (only once)
    validate_gpu_id "$gpu_id" >/dev/null || return 1
    
    log "Restarting service for GPU $gpu_id..."
    service comfyui restart "$gpu_id"
}

# Function to restart all GPU services
restart_all_services() {
    local failed=0
    for gpu in $(seq 0 $((NUM_GPUS-1))); do
        if ! restart_service "$gpu"; then
            failed=1
        fi
    done
    return $failed
}

# Function to check status of a specific GPU service
check_status() {
    local gpu_id=$1
    
    if [ "$gpu_id" = "all" ]; then
        check_all_status
        return $?
    fi
    
    # Validate GPU ID first (only once)
    validate_gpu_id "$gpu_id" >/dev/null || return 1
    
    # Capture the output of comfyui status
    local status_output
    status_output=$(service comfyui status "$gpu_id" 2>&1)
    local exit_code=$?
    
    # Extract status from the output (matches "Service is running" or "Service is not running")
    if echo "$status_output" | grep -q "Service is running"; then
        printf "GPU %d: %s\n" "$gpu_id" "$(echo "$status_output" | grep "Service is" | sed "s/\[.*\] //")"
        return 0
    else
        printf "GPU %d: %s\n" "$gpu_id" "$(echo "$status_output" | grep "Service is" | sed "s/\[.*\] //")"
        return $exit_code
    fi
}

# Function to check status of all GPU services
check_all_status() {
    local failed=0
    for gpu in $(seq 0 $((NUM_GPUS-1))); do
        if ! check_status "$gpu"; then
            failed=1
        fi
    done
    return $failed
}

# Function to setup ComfyUI for a specific GPU or CPU
setup_gpu() {
    local gpu_id=$1
    local target_dir
    
    if [ "$gpu_id" = "all" ]; then
        setup_all_gpus
        return $?
    elif [ "$gpu_id" = "cpu" ]; then
        target_dir="${ROOT}/comfyui_cpu"
    else
        target_dir="${ROOT}/comfyui_gpu${gpu_id}"
    fi
    
    log "Setting up ComfyUI for ${gpu_id}..."
    
    # Create directory if it doesn't exist
    mkdir -p "$target_dir"
    chmod 755 "$target_dir"
    
    # Clone or update ComfyUI
    if [ ! -d "${target_dir}/.git" ]; then
        # Fresh clone if not a git repo
        log "Cloning ComfyUI for ${gpu_id}..."
        if ! git clone --depth 1 https://github.com/comfyanonymous/ComfyUI.git "$target_dir"; then
            log "ERROR: Failed to clone ComfyUI"
            return 1
        fi
    else
        # Update existing repo
        log "Updating existing ComfyUI repo for ${gpu_id}..."
        (cd "$target_dir" && git pull) || {
            log "ERROR: Failed to update ComfyUI"
            return 1
        }
    fi
    
    # Create logs directory
    mkdir -p "${target_dir}/logs"
    chmod 755 "${target_dir}/logs"
    touch "${target_dir}/logs/output.log"
    chmod 644 "${target_dir}/logs/output.log"
    
    # Verify setup
    if [ ! -f "${target_dir}/main.py" ]; then
        log "ERROR: Setup failed - main.py not found in $target_dir"
        return 1
    fi
    
    log "Setup complete for ${gpu_id}"
    return 0
}

# Function to setup all GPU directories
setup_all_gpus() {
    local failed=0
    for gpu in $(seq 0 $((NUM_GPUS-1))); do
        setup_gpu "$gpu" || failed=1
    done
    return $failed
}

# Show usage information
show_usage() {
    echo "Usage: $0 COMMAND [GPU_ID|all|cpu]"
    echo
    echo "Commands:"
    echo "  start <gpu_id|all>    Start service for specific GPU or all GPUs"
    echo "  stop <gpu_id|all>     Stop service for specific GPU or all GPUs"
    echo "  restart <gpu_id|all>  Restart service for specific GPU or all GPUs"
    echo "  status <gpu_id|all>   Show status for specific GPU or all GPUs"
    echo "  logs <gpu_id|all>     Show logs for specific GPU or all GPUs"
    echo "  setup <gpu_id|all|cpu> Setup ComfyUI for specific GPU, all GPUs, or CPU mode"
    echo "  count                 Show number of configured GPUs and info"
    echo
    echo "Examples:"
    echo "  $0 start 0            Start ComfyUI service for GPU 0"
    echo "  $0 start all          Start ComfyUI services for all GPUs"
    echo "  $0 setup cpu          Setup ComfyUI for CPU-only mode"
    echo "  $0 count              Show GPU configuration"
}

# Function to show GPU count
show_count() {
    if [ "${MOCK_GPU:-0}" = "1" ]; then
        echo "${NUM_GPUS}"
    else
        # Try to get real GPU count from nvidia-smi
        if command -v nvidia-smi >/dev/null 2>&1; then
            nvidia-smi --query-gpu=gpu_name --format=csv,noheader | wc -l
        else
            echo "0"
        fi
    fi
}

# Main command handling
case "$1" in
    start|stop|restart|status|logs|setup)
        if [ -z "$2" ]; then
            show_usage
            exit 1
        fi
        
        if ! validate_gpu_id "$2"; then
            exit 1
        fi
        
        case "$1" in
            start)    start_service "$2" ;;
            stop)     stop_service "$2" ;;
            restart)  restart_service "$2" ;;
            status)   check_status "$2" ;;
            logs)     show_logs "$2" ;;
            setup)    setup_gpu "$2" ;;
        esac
        ;;
    count)
        show_count
        ;;
    *)
        show_usage
        exit 1
        ;;
esac